{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageFile\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agaricus',\n",
       " 'Amanita',\n",
       " 'Boletus',\n",
       " 'Cortinarius',\n",
       " 'Entoloma',\n",
       " 'Hygrocybe',\n",
       " 'Lactarius',\n",
       " 'Russula',\n",
       " 'Suillus']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = 'Archive/Partitioned_Dataset/train'\n",
    "path = Path(base_dir)\n",
    "\n",
    "classes  = [entry.name for entry in path.iterdir() if entry.is_dir()]\n",
    "classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Set up the directories\n",
    "base_dir = 'Archive/Partitioned_Dataset'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validate')\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 11.2 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.725    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25e9009aa9f42668d335cf8869c5a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769abb10f797491f9c53a4c04cfc2c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "class ResNet18FeatureExtractor(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # Freeze all the layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the final fully connected layer\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = torch.sum(preds == labels).item() / len(labels)\n",
    "        self.val_losses.append(loss)\n",
    "        self.val_accs.append(acc)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.val_losses).mean()\n",
    "        avg_acc = torch.tensor(self.val_accs).mean()\n",
    "        self.log('val_loss', avg_loss)\n",
    "        self.log('val_acc', avg_acc)\n",
    "        self.val_losses.clear()\n",
    "        self.val_accs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    dirpath='checkpoints',\n",
    "    filename='mushroom-classifier-{epoch:02d}-{val_acc:.2f}'\n",
    ")\n",
    "\n",
    "# Training\n",
    "model = ResNet18FeatureExtractor(num_classes=9)  # Replace with your number of classes\n",
    "trainer = pl.Trainer(max_epochs=10,    \n",
    "                     #accelerator='cpu',  # Explicitly specify CPU usage\n",
    "                     #devices=1,          # Use one CPU device\n",
    "                     callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18FineTuner(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18FineTuner, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # Unfreeze the last few layers\n",
    "        for param in self.model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = torch.sum(preds == labels).item() / len(labels)\n",
    "        self.val_losses.append(loss)\n",
    "        self.val_accs.append(acc)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.val_losses).mean()\n",
    "        avg_acc = torch.tensor(self.val_accs).mean()\n",
    "        self.log('val_loss', avg_loss)\n",
    "        self.log('val_acc', avg_acc)\n",
    "        self.val_losses.clear()\n",
    "        self.val_accs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "model = ResNet18FineTuner(num_classes=10)  # Replace with your number of classes\n",
    "trainer = pl.Trainer(max_epochs=5)  # Additional epochs for fine-tuning\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 11.2 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.725    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd4bc3df1bb430fb491179a8a41826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd3cd800b334af59b0ef65b12687d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbe333c81b34359b0f559954064eb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 284: 'val_acc' reached 0.81189 (best 0.81189), saving model to 'C:\\\\Users\\\\Gintare\\\\Desktop\\\\Study\\\\Module4\\\\Sprint1\\\\Mushrooms_classification\\\\checkpoints\\\\mushroom-classifier-epoch=00-val_acc=0.81.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef0f2cdf00f480d90d3ccc4070ba071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 568: 'val_acc' reached 0.88165 (best 0.88165), saving model to 'C:\\\\Users\\\\Gintare\\\\Desktop\\\\Study\\\\Module4\\\\Sprint1\\\\Mushrooms_classification\\\\checkpoints\\\\mushroom-classifier-epoch=01-val_acc=0.88.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebb6bcb23f94cd7839cae8227f810f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 852: 'val_acc' reached 0.88992 (best 0.88992), saving model to 'C:\\\\Users\\\\Gintare\\\\Desktop\\\\Study\\\\Module4\\\\Sprint1\\\\Mushrooms_classification\\\\checkpoints\\\\mushroom-classifier-epoch=02-val_acc=0.89.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "# Define the data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Set up the directories\n",
    "base_dir = 'Archive/Partitioned_Dataset'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validate')\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# Define the model\n",
    "class MushroomClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(MushroomClassifier, self).__init__()\n",
    "        self.model = models.resnet18(weights=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "# Initialize the model, trainer, and start training\n",
    "model = MushroomClassifier(num_classes=len(train_dataset.classes))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    dirpath='checkpoints',\n",
    "    filename='mushroom-classifier-{epoch:02d}-{val_acc:.2f}'\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=25,\n",
    "    accelerator='cpu',  # Explicitly specify CPU usage\n",
    "    devices=1,          # Use one CPU device\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
